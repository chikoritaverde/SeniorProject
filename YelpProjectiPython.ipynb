{
 "metadata": {
  "name": "",
  "signature": "sha256:26231f3590d964514a671601dcbfeb98aa08df95cfdfdc1c1ef3a82b3447ce3f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Acknowledgements for code used for templates and modeling\n",
      "\n",
      "Scikit-Lean Libraries and Documentation:\n",
      "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
      "\n",
      "Zacstewart's github: \n",
      "Text Classification with Scikit Learn https://gist.github.com/zacstewart/5978000 Copyright 2014\n",
      "\n",
      "Randomized Logistic Regression - http://scikit-learn.org/0.13/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html for feature selection\n",
      "\n",
      "Most informative features\n",
      "http://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import pandas\n",
      "import matplotlib as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "import math\n",
      "import numpy as np\n",
      "from scipy.stats import expon as ex\n",
      "import nltk\n",
      "from datetime import datetime\n",
      "import re\n",
      "from sklearn import svm\n",
      "import csv\n",
      "import sys\n",
      "import time\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn import cross_validation\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn import linear_model\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn import tree\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import linear_model\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.externals.six import StringIO  \n",
      "import pydot\n",
      "import os\n",
      "from sklearn import preprocessing\n",
      "from sklearn.cross_validation import KFold\n",
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "#building machine learning systems with python book - investigate"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Couldn't import dot_parser, loading of dot files will not be possible.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reviews = {}\n",
      "users = {}\n",
      "businesses = {}\n",
      "ids = []\n",
      "features=[]\n",
      "target = []\n",
      "featurefile = \"features_1417670000.26.csv\"\n",
      "targetfile = \"target_1417669996.94.csv\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "business_file = open('/Users/Dania/Desktop/YelpDatasetChallenge/yelp_dataset_challenge_academic_dataset/data_business.json')\n",
      "for line in business_file:\n",
      "    biz = json.loads(line)\n",
      "    biz_id = biz[u'business_id']\n",
      "    businesses[biz_id] = biz\n",
      "print(len(businesses))\n",
      "business_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "42153\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "review_file = open('/Users/Dania/Desktop/YelpDatasetChallenge/yelp_dataset_challenge_academic_dataset/data_review.json')\n",
      "for line in review_file:\n",
      "    review=json.loads(line)\n",
      "    rev_id=review[u'review_id']\n",
      "    ids.append(rev_id)\n",
      "    reviews[rev_id]=review\n",
      "print len(reviews)\n",
      "review_file.close()    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1125458\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_file = open('/Users/Dania/Desktop/YelpDatasetChallenge/yelp_dataset_challenge_academic_dataset/data_user.json')\n",
      "for line in user_file:\n",
      "    user=json.loads(line)\n",
      "    u_id=user[u'user_id']\n",
      "    users[u_id]=user\n",
      "print len(users)    \n",
      "user_file.close()    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "252898\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reviewAgeCalc(day):\n",
      "    dt=datetime.strptime(day,'%Y-%m-%d')\n",
      "    return ((datetime.now().date()-dt.date()).total_seconds()/86400)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#collect and tokenize text, further cleaning could be done here\n",
      "corpus = []\n",
      "for rid in ids:\n",
      "    review = reviews[rid]\n",
      "    text = (review[u'text'])\n",
      "    text = text.lower()\n",
      "    text = re.sub('\\!+', \"!\", text)\n",
      "    text = re.sub('\\.+', \".\", text)\n",
      "    text = re.sub('\\?+', \"?\", text)\n",
      "    text = re.sub('(\\\\n)+', '\\n', text)\n",
      "    reviews[rid][u'text']=text\n",
      "    corpus.append(text)\n",
      "    \n",
      "corpus = np.array(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reviewTextMetadata(text):\n",
      "    #clean text\n",
      "    tokens=nltk.word_tokenize(text)\n",
      "    count_stop= tokens.count('?') + tokens.count(\".\") + tokens.count(\"!\")\n",
      "    count_stop=max(count_stop, 1)\n",
      "    average_sentence_length=len(tokens)/count_stop\n",
      "    review_length=len(tokens)\n",
      "    paragraph_count=text.count('\\\\n')\n",
      "    \n",
      "    return review_length, average_sentence_length, paragraph_count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Order for feature vector:\n",
      "    From review\n",
      "        1. Review Length\n",
      "        2. Average Sentence Length\n",
      "        3. Review Stars\n",
      "        4. Paragraph Count\n",
      "        5. Age of Review (days)    \n",
      "    From business\n",
      "        6. Overall Business Stars\n",
      "        7. Review Count\n",
      "    From business and review    \n",
      "        8. Difference between review stars and business stars\n",
      "    From user\n",
      "        9. Number of friends\n",
      "        10. Count of reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#do once, then can bring back with csv files\n",
      "def generateFeatureMatrix():\n",
      "    features=[]\n",
      "    target=[]\n",
      "    for rid in ids:\n",
      "        #from review\n",
      "        review=reviews[rid]\n",
      "        target.append(review[u'votes'][u'useful']) #adding to target vector\n",
      "        reviewLength, reviewAverageSentenceLength, reviewParagraphCount = reviewTextMetadata(review[u'text'])\n",
      "        reviewStars = review[u'stars']\n",
      "        reviewAge = reviewAgeCalc(review[u'date'])\n",
      "        #from business\n",
      "        bid=review[u'business_id']\n",
      "        business=businesses[bid]\n",
      "        businessStars=business[u'stars']\n",
      "        businessReviewCount=business[u'review_count']\n",
      "        starDifference = (reviewStars-businessStars)\n",
      "        #from user\n",
      "        uid=review[u'user_id']\n",
      "        userFriends=len(users[uid][u'friends'])\n",
      "        userReviewCount=users[uid][u'review_count']\n",
      "        featureArray=[reviewLength,\n",
      "                      reviewAverageSentenceLength, \n",
      "                      reviewStars,\n",
      "                      reviewParagraphCount,\n",
      "                      reviewAge,\n",
      "                      businessStars, \n",
      "                      businessReviewCount, \n",
      "                      starDifference,\n",
      "                      userFriends,\n",
      "                      userReviewCount]\n",
      "        features.append(featureArray)\n",
      "        if(len(features)!=len(target)):\n",
      "            print len(features)\n",
      "            print len (target)\n",
      "            print review\n",
      "            break\n",
      "\n",
      "    print len(features) \n",
      "    \n",
      "    ts=time.time()\n",
      "    featuresFilename = 'features_' + str(ts) + \".csv\"\n",
      "    f=open(featuresFilename, 'wb')\n",
      "    writer=csv.writer(f)\n",
      "    writer.writerows(features)\n",
      "    f.close()\n",
      "    print featuresFilename\n",
      "    \n",
      "    targetFilename = 'target_' + str(ts) + \".csv\"\n",
      "    writeTargetFile(target, targetFilename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def writeTargetFile(targetVector, filename):\n",
      "    ts=time.time() \n",
      "    f=open(filename, 'wb')\n",
      "    target1=target[0:10]\n",
      "    writer=csv.writer(f, quoting=csv.QUOTE_ALL)\n",
      "    writer.writerow(targetVector)\n",
      "    f.close()\n",
      "    print filename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def importFeatureMatrixFromCSV(featurefile, targetfile):\n",
      "    features=[]\n",
      "    with open(featurefile, 'rb') as f:\n",
      "        reader=csv.reader(f)\n",
      "        for row in reader:\n",
      "            frow=[]\n",
      "            for item in row:\n",
      "                f= float(item)\n",
      "                frow.append(f)\n",
      "            features.append(frow)\n",
      "    print len(features) #confirm correct functionality \n",
      "        \n",
      "    target = importTargetVectorFromCSV(targetfile)\n",
      "    \n",
      "    return features, target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def importTargetVectorFromCSV(targetfile):\n",
      "    target=[]\n",
      "    with open(targetfile, 'rb') as f:\n",
      "        reader=csv.reader(f)\n",
      "        for row in reader:\n",
      "            for item in row:\n",
      "                t= int(item)\n",
      "                target.append(t)\n",
      "    print len(target) #confirm correct functionality\n",
      "    \n",
      "    return target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features, target = importFeatureMatrixFromCSV(featurefile, targetfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1125458\n",
        "1125458"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Constants for data partition\n",
      "TOTAL_REVIEW_COUNT = 1125458-1\n",
      "FIRST_QUARTER= 281354  \n",
      "FIRST_THIRD = 375152  \n",
      "HALF=562729   \n",
      "TWO_THIRDS = 750305 \n",
      "THIRD_QUARTER = 844093\n",
      "FOUR_FIFTHS = 894740\n",
      "Column_titles = ['revlength', 'avsentlength', 'revstar', 'paragraphcount', 'age', 'bizstar', 'bizrevcount', 'stardiff', 'numfriends', 'userReviews']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write features to CSV files for safe keeping"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create target file that treats useful as binary - ie. a review receives any number of useful votes or it doesn't\n",
      "targetBinary=[]\n",
      "for t in target:\n",
      "    if t>0:\n",
      "        targetBinary.append(1)\n",
      "    else:\n",
      "        targetBinary.append(0)\n",
      "print len(targetBinary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1125458\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featureMatrix = np.mat(features)\n",
      "\n",
      "featuresTrain = featureMatrix[0:TWO_THIRDS-1]\n",
      "featuresTest = featureMatrix[TWO_THIRDS:TOTAL_REVIEW_COUNT]\n",
      "targetTrain = target[0:TWO_THIRDS-1]\n",
      "targetTest = target[TWO_THIRDS: TOTAL_REVIEW_COUNT]\n",
      "targetBinaryTrain = targetBinary[0:TWO_THIRDS-1]\n",
      "targetBinaryTest = targetBinary[TWO_THIRDS: TOTAL_REVIEW_COUNT]\n",
      "\n",
      "featuresSplitTest = featureMatrix[TWO_THIRDS: FOUR_FIFTHS-1]\n",
      "targetBinarySplitTest = targetBinary[TWO_THIRDS: FOUR_FIFTHS-1]\n",
      "featuresSplitValidation = featureMatrix[FOUR_FIFTHS: TOTAL_REVIEW_COUNT]\n",
      "targetBinarySplitValidation = targetBinary[FOUR_FIFTHS: TOTAL_REVIEW_COUNT]\n",
      "\n",
      "corpusTrain = corpus[0:TWO_THIRDS-1]\n",
      "corpusTest = corpus[TWO_THIRDS:TOTAL_REVIEW_COUNT]\n",
      "corpusSplitTest = corpus[TWO_THIRDS:FOUR_FIFTHS-1]\n",
      "corpusSplitValidation = corpus[FOUR_FIFTHS:TOTAL_REVIEW_COUNT]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpusMiniTrain = corpusTrain[0:FIRST_THIRD-1]\n",
      "targetBinaryMiniTrain = targetBinaryTrain[0:FIRST_THIRD-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "writeTargetFile(targetBinarySplitTest, \"TargetTrainingforD3.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TargetTrainingforD3.csv\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#train tree with no normalization\n",
      "dtc = tree.DecisionTreeClassifier()\n",
      "dtc.fit(featuresTrain, targetTrain)\n",
      "dtc_test = dtc.predict(featuresTrain)\n",
      "print(classification_report(targetTrain, dtc_test))\n",
      "dtc_cross_test = dtc.predict(featuresTest)\n",
      "print(classification_report(targetTest, dtc_cross_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00    388144\n",
        "          1       1.00      1.00      1.00    183949\n",
        "          2       1.00      1.00      1.00     82831\n",
        "          3       1.00      1.00      1.00     39352\n",
        "          4       1.00      1.00      1.00     20384\n",
        "          5       1.00      1.00      1.00     11552\n",
        "          6       1.00      1.00      1.00      7031\n",
        "          7       1.00      1.00      1.00      4619\n",
        "          8       1.00      1.00      1.00      3096\n",
        "          9       1.00      1.00      1.00      2159\n",
        "         10       1.00      1.00      1.00      1558\n",
        "         11       1.00      1.00      1.00      1181\n",
        "         12       1.00      1.00      1.00       908\n",
        "         13       1.00      1.00      1.00       741\n",
        "         14       1.00      1.00      1.00       542\n",
        "         15       1.00      1.00      1.00       419\n",
        "         16       1.00      1.00      1.00       312\n",
        "         17       1.00      1.00      1.00       281\n",
        "         18       1.00      1.00      1.00       214\n",
        "         19       1.00      1.00      1.00       178\n",
        "         20       1.00      1.00      1.00       129\n",
        "         21       1.00      1.00      1.00       113\n",
        "         22       1.00      1.00      1.00        75\n",
        "         23       1.00      1.00      1.00        70\n",
        "         24       1.00      1.00      1.00        75\n",
        "         25       1.00      1.00      1.00        60\n",
        "         26       1.00      1.00      1.00        40\n",
        "         27       1.00      1.00      1.00        34\n",
        "         28       1.00      1.00      1.00        34\n",
        "         29       1.00      1.00      1.00        28\n",
        "         30       1.00      1.00      1.00        18\n",
        "         31       1.00      1.00      1.00        27\n",
        "         32       1.00      1.00      1.00        19\n",
        "         33       1.00      1.00      1.00        11\n",
        "         34       1.00      1.00      1.00        13\n",
        "         35       1.00      1.00      1.00         9\n",
        "         36       1.00      1.00      1.00         6\n",
        "         37       1.00      1.00      1.00         6\n",
        "         38       1.00      1.00      1.00        15\n",
        "         39       1.00      1.00      1.00        10\n",
        "         40       1.00      1.00      1.00         8\n",
        "         41       1.00      1.00      1.00         3\n",
        "         42       1.00      1.00      1.00         4\n",
        "         43       1.00      1.00      1.00         2\n",
        "         44       1.00      1.00      1.00         6\n",
        "         45       1.00      1.00      1.00         6\n",
        "         46       1.00      1.00      1.00         3\n",
        "         47       1.00      1.00      1.00         4\n",
        "         48       1.00      1.00      1.00         2\n",
        "         49       1.00      1.00      1.00         2\n",
        "         50       1.00      1.00      1.00         2\n",
        "         51       1.00      1.00      1.00         2\n",
        "         52       1.00      1.00      1.00         3\n",
        "         53       1.00      1.00      1.00         2\n",
        "         55       1.00      1.00      1.00         1\n",
        "         56       1.00      1.00      1.00         2\n",
        "         57       1.00      1.00      1.00         1\n",
        "         58       1.00      1.00      1.00         1\n",
        "         61       1.00      1.00      1.00         1\n",
        "         64       1.00      1.00      1.00         1\n",
        "         65       1.00      1.00      1.00         1\n",
        "         66       1.00      1.00      1.00         1\n",
        "         67       1.00      1.00      1.00         1\n",
        "         70       1.00      1.00      1.00         1\n",
        "        134       1.00      1.00      1.00         1\n",
        "\n",
        "avg / total       1.00      1.00      1.00    750304\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.60      0.63      0.61    187794\n",
        "          1       0.27      0.27      0.27     92816\n",
        "          2       0.15      0.14      0.15     43102\n",
        "          3       0.10      0.09      0.10     20889\n",
        "          4       0.08      0.07      0.08     10803\n",
        "          5       0.07      0.05      0.06      6109\n",
        "          6       0.07      0.06      0.06      3904\n",
        "          7       0.04      0.03      0.04      2433\n",
        "          8       0.05      0.04      0.04      1747\n",
        "          9       0.04      0.03      0.03      1232\n",
        "         10       0.04      0.03      0.03       876\n",
        "         11       0.05      0.04      0.04       703\n",
        "         12       0.05      0.03      0.04       481\n",
        "         13       0.05      0.03      0.04       414\n",
        "         14       0.02      0.02      0.02       308\n",
        "         15       0.05      0.04      0.04       276\n",
        "         16       0.03      0.02      0.03       218\n",
        "         17       0.01      0.01      0.01       167\n",
        "         18       0.05      0.04      0.04       134\n",
        "         19       0.05      0.03      0.04       106\n",
        "         20       0.00      0.00      0.00        82\n",
        "         21       0.07      0.07      0.07        59\n",
        "         22       0.06      0.03      0.04        66\n",
        "         23       0.02      0.02      0.02        55\n",
        "         24       0.02      0.02      0.02        58\n",
        "         25       0.03      0.02      0.03        42\n",
        "         26       0.00      0.00      0.00        27\n",
        "         27       0.00      0.00      0.00        28\n",
        "         28       0.00      0.00      0.00        23\n",
        "         29       0.05      0.04      0.04        24\n",
        "         30       0.00      0.00      0.00        24\n",
        "         31       0.00      0.00      0.00        11\n",
        "         32       0.00      0.00      0.00        17\n",
        "         33       0.00      0.00      0.00        13\n",
        "         34       0.00      0.00      0.00         8\n",
        "         35       0.00      0.00      0.00         3\n",
        "         36       0.00      0.00      0.00        10\n",
        "         37       0.00      0.00      0.00         4\n",
        "         38       0.00      0.00      0.00         9\n",
        "         39       0.00      0.00      0.00         4\n",
        "         40       0.00      0.00      0.00         8\n",
        "         41       0.00      0.00      0.00         6\n",
        "         42       0.00      0.00      0.00         3\n",
        "         43       0.00      0.00      0.00         5\n",
        "         44       0.00      0.00      0.00         1\n",
        "         45       0.00      0.00      0.00         5\n",
        "         46       0.00      0.00      0.00         5\n",
        "         47       0.00      0.00      0.00         5\n",
        "         48       0.00      0.00      0.00         3\n",
        "         49       0.00      0.00      0.00         4\n",
        "         50       0.00      0.00      0.00         0\n",
        "         51       0.00      0.00      0.00         1\n",
        "         52       0.00      0.00      0.00         2\n",
        "         53       0.00      0.00      0.00         2\n",
        "         54       0.00      0.00      0.00         3\n",
        "         55       0.00      0.00      0.00         1\n",
        "         56       0.00      0.00      0.00         2\n",
        "         57       0.00      0.00      0.00         6\n",
        "         60       0.00      0.00      0.00         1\n",
        "         61       0.00      0.00      0.00         0\n",
        "         62       0.00      0.00      0.00         1\n",
        "         63       0.00      0.00      0.00         1\n",
        "         64       0.00      0.00      0.00         1\n",
        "         66       0.00      0.00      0.00         1\n",
        "         67       0.00      0.00      0.00         0\n",
        "         72       0.00      0.00      0.00         1\n",
        "         78       0.00      0.00      0.00         1\n",
        "         88       0.00      0.00      0.00         1\n",
        "        101       0.00      0.00      0.00         1\n",
        "        110       0.00      0.00      0.00         1\n",
        "        166       0.00      0.00      0.00         1\n",
        "\n",
        "avg / total       0.39      0.41      0.40    375152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n",
        "/Library/Python/2.7/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
        "  'recall', 'true', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#train tree with no normalization and binary target\n",
      "dtcb = tree.DecisionTreeClassifier()\n",
      "dtcb.fit(featuresTrain, targetBinaryTrain)\n",
      "dtc_test = dtcb.predict(featuresTrain)\n",
      "print(classification_report(targetBinaryTrain, dtc_test))\n",
      "dtc_cross_test = dtcb.predict(featuresTest)\n",
      "print(classification_report(targetBinaryTest, dtc_cross_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00    388144\n",
        "          1       1.00      1.00      1.00    362160\n",
        "\n",
        "avg / total       1.00      1.00      1.00    750304\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.59      0.65      0.62    187794\n",
        "          1       0.61      0.55      0.58    187358\n",
        "\n",
        "avg / total       0.60      0.60      0.60    375152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtc_reducedFeatures = tree.DecisionTreeClassifier()\n",
      "featureReducedTrain = np.delete(featuresTrain, np.s_[2:4],1)\n",
      "featureReducedTest = np.delete(featuresTest, np.s_[2:4],1)\n",
      "\n",
      "print np.shape(featureReducedTrain)\n",
      "dtc_reducedFeatures.fit(featureReducedTrain, targetBinaryTrain)\n",
      "dtcbr_test = dtc_reducedFeatures.predict(featureReducedTrain)\n",
      "print classification_report(targetBinaryTrain, dtcbr_test)\n",
      "dtcbr_cross_test = dtc_reducedFeatures.predict(featureReducedTest)\n",
      "print classification_report(targetBinaryTest, dtcbr_cross_test)\n",
      "print dtc_reducedFeatures.feature_importances_\n",
      "\n",
      "dtcbr_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(750304, 8)\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00    388144\n",
        "          1       1.00      1.00      1.00    362160\n",
        "\n",
        "avg / total       1.00      1.00      1.00    750304\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.59      0.65      0.62    187794\n",
        "          1       0.61      0.55      0.58    187358\n",
        "\n",
        "avg / total       0.60      0.60      0.60    375152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 0.17292024  0.09492905  0.18511099  0.03767026  0.15604489  0.05253086\n",
        "  0.16730404  0.13348967]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtc_normalized = tree.DecisionTreeClassifier()\n",
      "#normalizedTrain = preprocessing.normalize(featuresTrain)\n",
      "#normalizedTest = preprocessing.normalize(featuresTest)\n",
      "dtc_normalized.fit(normalizedTrain, targetBinaryTrain)\n",
      "dtc_normalized_test = dtc_normalized.predict(featuresTrain)\n",
      "print classification_report(targetBinaryTrain, dtc_normalized_test)\n",
      "dtc_normalized_cross_test = dtc_normalized.predict(normalizedTest)\n",
      "print classification_report(targetBinaryTest, dtc_normalized_cross_test)\n",
      "print dtc_normalized.feature_importances_\n",
      "\n",
      "dtc_normalized_validate_test = dtc_normalized.predict(featuresSplitTest)\n",
      "print classification_report(targetBinarySplitTest, dtc_normalized_validate_test)\n",
      "writeTargetFile(dtc_normalized_validate_test, \"DecisionTreeMetadata_NormalizedforD3.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.66      0.16      0.26    388144\n",
        "          1       0.50      0.91      0.65    362160\n",
        "\n",
        "avg / total       0.59      0.52      0.45    750304\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.58      0.64      0.61    187794\n",
        "          1       0.60      0.54      0.57    187358\n",
        "\n",
        "avg / total       0.59      0.59      0.59    375152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 0.12987938  0.10704854  0.10384226  0.          0.091717    0.10975393\n",
        "  0.09445028  0.07520608  0.18053919  0.10756333]\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.63      0.17      0.26     72727\n",
        "          1       0.52      0.90      0.66     71707\n",
        "\n",
        "avg / total       0.57      0.53      0.46    144434\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "DecisionTreeMetadata_NormalizedforD3.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#information about this machine\n",
      "print dtcb\n",
      "print dtcb.tree_.feature\n",
      "print dtcb.tree_.value\n",
      "print dtcb.feature_importances_\n",
      "#This tree's top three features: Age of review, review length, number of friends of reviewer\n",
      "#close fourth: number of total reviews for business\n",
      "#no single feature is very telling."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best')\n",
        "[ 8  0  4 ...,  4 -2 -2]\n",
        "[[[ 0.  0.]]\n",
        "\n",
        " [[ 0.  0.]]\n",
        "\n",
        " [[ 0.  0.]]\n",
        "\n",
        " ..., \n",
        " [[ 0.  0.]]\n",
        "\n",
        " [[ 1.  0.]]\n",
        "\n",
        " [[ 0.  2.]]]\n",
        "[ 0.17369614  0.0935168   0.02307118  0.          0.18587575  0.03182445\n",
        "  0.15558431  0.03897777  0.16793368  0.12951992]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtc_p1 = tree.DecisionTreeClassifier(max_depth=10)\n",
      "dtc_p1.fit(featuresTrain, targetBinaryTrain)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=10, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best')"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtc_p1_test = dtc_p1.predict(featuresTrain)\n",
      "print \"Accuracy on training set\"\n",
      "print classification_report(targetBinaryTrain, dtc_p1_test)\n",
      "\n",
      "print \"Accuracy on full test set\"\n",
      "dtc_p1_cross_test = dtc_p1.predict(featuresTest)\n",
      "print classification_report(targetBinaryTest, dtc_p1_cross_test)\n",
      "\n",
      "print \"Accuracy on half test set\"\n",
      "dtc_p1_validate_test = dtc_p1.predict(featuresSplitTest)\n",
      "print classification_report(targetBinarySplitTest, dtc_p1_validate_test)\n",
      "\n",
      "print \"Accuracy on validation set\"\n",
      "dtc_p1_validate_val = dtc_p1.predict(featuresSplitValidation)\n",
      "print classification_report(targetBinarySplitValidation, dtc_p1_validate_val)\n",
      "\n",
      "print \"Feature Importance\"\n",
      "print dtc_p1.feature_importances_\n",
      "\n",
      "with open(\"/Users/Dania/Desktop/dtree.dot\", 'w') as f:\n",
      "    f = tree.export_graphviz(dtc_p1, out_file=f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy on training set\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.68      0.75      0.71    388144\n",
        "          1       0.70      0.62      0.66    362160\n",
        "\n",
        "avg / total       0.69      0.69      0.69    750304\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on full test set\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.63      0.81      0.71    187794\n",
        "          1       0.73      0.53      0.62    187358\n",
        "\n",
        "avg / total       0.68      0.67      0.66    375152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on half test set\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.65      0.77      0.71     72727\n",
        "          1       0.71      0.58      0.64     71707\n",
        "\n",
        "avg / total       0.68      0.68      0.67    144434\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on validation set\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.63      0.83      0.71    115066\n",
        "          1       0.75      0.51      0.60    115651\n",
        "\n",
        "avg / total       0.69      0.67      0.66    230717\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Feature Importance\n",
        "[ 0.23075609  0.00163537  0.03748111  0.          0.09317555  0.00423853\n",
        "  0.03605383  0.004013    0.5601541   0.0324924 ]\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "writeTargetFile(dtc_p1_validate_test, \"DecisionTreeMetadata_MaxDepthforD3.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DecisionTreeMetadata_MaxDepthforD3.csv\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Bernoulli Naive-Bayes\n",
      "#The Bernoulli version of Naive Bayes requires a binary outcome. \n",
      "bnb = BernoulliNB()\n",
      "bnb.fit(featuresTrain, targetBinaryTrain)\n",
      "bnb_test = bnb.predict(featuresTrain)\n",
      "print(classification_report(targetBinaryTrain, bnb_test))\n",
      "bnb_cross_test = bnb.predict(featuresTest)\n",
      "print(classification_report(targetBinaryTest, bnb_cross_test))\n",
      "\n",
      "bnb_validate_test = bnb.predict(featuresSplitTest)\n",
      "print classification_report(targetBinarySplitTest, bnb_validate_test)\n",
      "\n",
      "writeTargetFile(bnb_validate_test, )\n",
      "\n",
      "print bnb\n",
      "print bnb.coef_\n",
      "print bnb.class_count_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.70      0.33      0.45    388144\n",
        "          1       0.54      0.85      0.66    362160\n",
        "\n",
        "avg / total       0.63      0.58      0.55    750304\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.66      0.39      0.49    187794\n",
        "          1       0.57      0.80      0.66    187358\n",
        "\n",
        "avg / total       0.61      0.60      0.58    375152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "[[ -2.76119908e-06  -2.76119908e-06  -2.76119908e-06  -1.27998469e+01\n",
        "   -2.76119908e-06  -2.76119908e-06  -2.76119908e-06  -7.78328306e-01\n",
        "   -1.58598257e-01  -2.76119908e-06]]\n",
        "[ 388144.  362160.]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Multinomial Naive Bayes throws an error whenever negative features are used. Currently, the Features Matrix contains \n",
      "#negative values in the difference field. This will be run with the absolute value of the Features Matrix\n",
      "mnb = MultinomialNB()\n",
      "absFeaturesTrain = np.absolute(featuresTrain)\n",
      "absFeaturesTest = np.absolute(featuresTest)\n",
      "mnb.fit(absFeaturesTrain, targetTrain)\n",
      "mnb_test = mnb.predict(absFeaturesTrain)\n",
      "print(classification_report(targetTrain, mnb_test))\n",
      "mnb_cross_test = mnb.predict(absFeaturesTest)\n",
      "print(classification_report(targetTest, mnb_cross_test))\n",
      "#resulted in really terrible accuracy! Completely inappropriate algorithm here. Can't even predict the training set well!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.63      0.13      0.22    388144\n",
        "          1       0.26      0.14      0.19    183949\n",
        "          2       0.14      0.13      0.13     82831\n",
        "          3       0.08      0.08      0.08     39352\n",
        "          4       0.05      0.04      0.04     20384\n",
        "          5       0.04      0.09      0.05     11552\n",
        "          6       0.02      0.01      0.01      7031\n",
        "          7       0.03      0.01      0.01      4619\n",
        "          8       0.01      0.01      0.01      3096\n",
        "          9       0.01      0.01      0.01      2159\n",
        "         10       0.01      0.02      0.02      1558\n",
        "         11       0.05      0.00      0.00      1181\n",
        "         12       0.01      0.00      0.01       908\n",
        "         13       0.01      0.09      0.02       741\n",
        "         14       0.00      0.00      0.00       542\n",
        "         15       0.01      0.00      0.00       419\n",
        "         16       0.00      0.00      0.00       312\n",
        "         17       0.00      0.00      0.00       281\n",
        "         18       0.00      0.00      0.00       214\n",
        "         19       0.00      0.00      0.00       178\n",
        "         20       0.00      0.00      0.00       129\n",
        "         21       0.00      0.00      0.00       113\n",
        "         22       0.00      0.00      0.00        75\n",
        "         23       0.00      0.00      0.00        70\n",
        "         24       0.00      0.00      0.00        75\n",
        "         25       0.14      0.02      0.03        60\n",
        "         26       0.00      0.00      0.00        40\n",
        "         27       0.00      0.00      0.00        34\n",
        "         28       0.00      0.00      0.00        34\n",
        "         29       0.00      0.14      0.00        28\n",
        "         30       0.00      0.11      0.00        18\n",
        "         31       0.00      0.00      0.00        27\n",
        "         32       0.00      0.05      0.00        19\n",
        "         33       0.01      0.09      0.02        11\n",
        "         34       0.00      0.00      0.00        13\n",
        "         35       0.00      0.00      0.00         9\n",
        "         36       0.00      0.00      0.00         6\n",
        "         37       0.00      0.00      0.00         6\n",
        "         38       0.00      0.00      0.00        15\n",
        "         39       0.00      0.10      0.00        10\n",
        "         40       0.00      0.00      0.00         8\n",
        "         41       0.00      0.00      0.00         3\n",
        "         42       0.00      0.25      0.00         4\n",
        "         43       0.00      0.00      0.00         2\n",
        "         44       0.00      0.00      0.00         6\n",
        "         45       0.00      0.00      0.00         6\n",
        "         46       0.00      0.00      0.00         3\n",
        "         47       0.00      0.50      0.00         4\n",
        "         48       0.00      0.00      0.00         2\n",
        "         49       0.00      0.00      0.00         2\n",
        "         50       0.00      0.50      0.00         2\n",
        "         51       0.00      0.00      0.00         2\n",
        "         52       0.00      0.00      0.00         3\n",
        "         53       0.00      0.00      0.00         2\n",
        "         55       0.00      1.00      0.00         1\n",
        "         56       0.00      0.00      0.00         2\n",
        "         57       0.00      1.00      0.00         1\n",
        "         58       0.02      1.00      0.04         1\n",
        "         61       0.01      1.00      0.03         1\n",
        "         64       0.00      1.00      0.00         1\n",
        "         65       0.00      1.00      0.00         1\n",
        "         66       0.00      1.00      0.00         1\n",
        "         67       0.00      1.00      0.00         1\n",
        "         70       0.01      1.00      0.01         1\n",
        "        134       0.00      1.00      0.00         1\n",
        "\n",
        "avg / total       0.41      0.12      0.18    750304\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.63      0.14      0.23    187794\n",
        "          1       0.26      0.17      0.20     92816\n",
        "          2       0.13      0.14      0.14     43102\n",
        "          3       0.07      0.07      0.07     20889\n",
        "          4       0.04      0.03      0.03     10803\n",
        "          5       0.02      0.05      0.03      6109\n",
        "          6       0.02      0.01      0.01      3904\n",
        "          7       0.01      0.00      0.01      2433\n",
        "          8       0.01      0.01      0.01      1747\n",
        "          9       0.00      0.00      0.00      1232\n",
        "         10       0.01      0.01      0.01       876\n",
        "         11       0.00      0.00      0.00       703\n",
        "         12       0.00      0.00      0.00       481\n",
        "         13       0.00      0.01      0.00       414\n",
        "         14       0.02      0.00      0.01       308\n",
        "         15       0.00      0.00      0.00       276\n",
        "         16       0.00      0.00      0.00       218\n",
        "         17       0.00      0.00      0.00       167\n",
        "         18       0.00      0.00      0.00       134\n",
        "         19       0.00      0.00      0.00       106\n",
        "         20       0.00      0.00      0.00        82\n",
        "         21       0.00      0.00      0.00        59\n",
        "         22       0.00      0.00      0.00        66\n",
        "         23       0.00      0.00      0.00        55\n",
        "         24       0.00      0.00      0.00        58\n",
        "         25       0.00      0.00      0.00        42\n",
        "         26       0.00      0.00      0.00        27\n",
        "         27       0.00      0.00      0.00        28\n",
        "         28       0.00      0.00      0.00        23\n",
        "         29       0.00      0.12      0.00        24\n",
        "         30       0.00      0.00      0.00        24\n",
        "         31       0.00      0.00      0.00        11\n",
        "         32       0.00      0.00      0.00        17\n",
        "         33       0.00      0.00      0.00        13\n",
        "         34       0.00      0.00      0.00         8\n",
        "         35       0.00      0.00      0.00         3\n",
        "         36       0.00      0.00      0.00        10\n",
        "         37       0.00      0.00      0.00         4\n",
        "         38       0.00      0.00      0.00         9\n",
        "         39       0.00      0.00      0.00         4\n",
        "         40       0.00      0.00      0.00         8\n",
        "         41       0.00      0.00      0.00         6\n",
        "         42       0.00      0.00      0.00         3\n",
        "         43       0.00      0.00      0.00         5\n",
        "         44       0.00      0.00      0.00         1\n",
        "         45       0.00      0.00      0.00         5\n",
        "         46       0.00      0.00      0.00         5\n",
        "         47       0.00      0.20      0.00         5\n",
        "         48       0.00      0.00      0.00         3\n",
        "         49       0.00      0.00      0.00         4\n",
        "         50       0.00      0.00      0.00         0\n",
        "         51       0.00      0.00      0.00         1\n",
        "         52       0.00      0.00      0.00         2\n",
        "         53       0.00      0.50      0.00         2\n",
        "         54       0.00      0.00      0.00         3\n",
        "         55       0.00      0.00      0.00         1\n",
        "         56       0.00      0.00      0.00         2\n",
        "         57       0.00      0.00      0.00         6\n",
        "         58       0.00      0.00      0.00         0\n",
        "         60       0.00      0.00      0.00         1\n",
        "         61       0.00      0.00      0.00         0\n",
        "         62       0.00      0.00      0.00         1\n",
        "         63       0.00      0.00      0.00         1\n",
        "         64       0.00      0.00      0.00         1\n",
        "         65       0.00      0.00      0.00         0\n",
        "         66       0.00      0.00      0.00         1\n",
        "         67       0.00      0.00      0.00         0\n",
        "         70       0.00      0.00      0.00         0\n",
        "         72       0.00      0.00      0.00         1\n",
        "         78       0.00      0.00      0.00         1\n",
        "         88       0.00      0.00      0.00         1\n",
        "        101       0.00      0.00      0.00         1\n",
        "        110       0.00      0.00      0.00         1\n",
        "        134       0.00      0.00      0.00         0\n",
        "        166       0.00      0.00      0.00         1\n",
        "\n",
        "avg / total       0.40      0.13      0.19    375152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
        "  'recall', 'true', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rfc = RandomForestClassifier(n_estimators = 8, max_depth = 10)\n",
      "rfc.fit(featuresTrain, targetBinaryTrain)\n",
      "\n",
      "rfc_test = rfc.predict(featuresTrain)\n",
      "print \"Accuracy on training set\"\n",
      "print classification_report(targetBinaryTrain, rfc_test)\n",
      "\n",
      "print \"Accuracy on full test set\"\n",
      "rfc_cross_test = rfc.predict(featuresTest)\n",
      "print classification_report(targetBinaryTest, rfc_cross_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy on training set\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.68      0.76      0.72    388144\n",
        "          1       0.71      0.62      0.66    362160\n",
        "\n",
        "avg / total       0.69      0.69      0.69    750304\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Accuracy on full test set\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.63      0.82      0.72    187794\n",
        "          1       0.75      0.52      0.61    187358\n",
        "\n",
        "avg / total       0.69      0.67      0.66    375152\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#December 3, need to figure out what problem is here\n",
      "lr = linear_model.LinearRegression()\n",
      "lr.fit(featuresTrain, targetTrain)\n",
      "lr_test = lr.predict(featuresTrain)\n",
      "print(classification_report(targetTrain, lr_test))\n",
      "lr_cross_test = lr.predict(featuresTest)\n",
      "print(classification_report(targetTest, lr_cross_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Support Vector Regressions - Run time too long\n",
      "#svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
      "svr_lin = SVR(kernel='linear', C=1e3)\n",
      "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
      "svr_rbf.fit(featuresTrain, targetTrain)\n",
      "svr_rbf_test = svr_rbf.predict(featuresTrain)\n",
      "print(classification_report(targetTrain, svr_rbf_test))\n",
      "svr_rbf_cross_test = svr_rbf.predict(featuresTest)\n",
      "print(classification_report(targetTest, svr_rbf_cross_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This part of the code looks to build classifiers solely from natural language processesing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LemmaTokenizer(object):\n",
      "  def __init__(self):\n",
      "    self.wnl = WordNetLemmatizer()\n",
      "  def __call__(self, doc):\n",
      "    return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "\n",
      "vectorizer = CountVectorizer(tokenizer = LemmaTokenizer(), stop_words ='english', min_df = 1)\n",
      "transformer = TfidfTransformer()\n",
      "sgd = linear_model.SGDClassifier()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline_mnb = Pipeline([('vectorizer',  CountVectorizer(min_df=1, stop_words='english')), ('transformer', TfidfTransformer()), \n",
      "  ('classifier',  MultinomialNB()) ])\n",
      "\n",
      "pipeline_mnb.fit(corpusMiniTrain, targetBinaryMiniTrain)\n",
      "score = pipeline_mnb.score(corpusSplitTest, targetBinarySplitTest)\n",
      "mnb_validate_test = pipeline_mnb.predict(corpusSplitTest)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print score\n",
      "print classification_report(targetBinarySplitTest, mnb_validate_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.593225971724\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.59      0.66      0.62     72727\n",
        "          1       0.60      0.53      0.56     71707\n",
        "\n",
        "avg / total       0.59      0.59      0.59    144434\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline_bnb = Pipeline([('vectorizer',  CountVectorizer(min_df=1, stop_words='english')), ('transformer', TfidfTransformer()), \n",
      "  ('classifier',  BernoulliNB()) ])\n",
      "\n",
      "pipeline_bnb.fit(corpusMiniTrain, targetBinaryMiniTrain)\n",
      "score_bnb = pipeline_mnb.score(corpusSplitTest, targetBinarySplitTest)\n",
      "text_bnb_validate_test = pipeline_mnb.predict(corpusSplitTest)\n",
      "\n",
      "print score_bnb\n",
      "print classification_report(targetBinarySplitTest, text_bnb_validate_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.593225971724\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.59      0.66      0.62     72727\n",
        "          1       0.60      0.53      0.56     71707\n",
        "\n",
        "avg / total       0.59      0.59      0.59    144434\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "writeTargetFile(text_bnb_validate_test, \"TextBernoulliNBforD3.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TextBernoulliNBforD3.csv\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bnb_final_vect = CountVectorizer(min_df=1, stop_words='english')\n",
      "bnb_final_model =  BernoulliNB()\n",
      "pipeline_bnb2 = Pipeline([('vectorizer', bnb_final_vect), ('transformer', TfidfTransformer()), \n",
      "  ('classifier', bnb_final_model) ])\n",
      "\n",
      "pipeline_bnb2.fit(corpusMiniTrain, targetBinaryMiniTrain)\n",
      "score_bnb2 = pipeline_bnb2.score(corpusSplitTest, targetBinarySplitTest)\n",
      "text_bnb2_validate_test = pipeline_bnb2.predict(corpusSplitTest)\n",
      "\n",
      "print score_bnb2\n",
      "print classification_report(targetBinarySplitTest, text_bnb2_validate_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.623932038163\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.60      0.74      0.67     72727\n",
        "          1       0.66      0.50      0.57     71707\n",
        "\n",
        "avg / total       0.63      0.62      0.62    144434\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(bnb_final_vect.vocabulary_)\n",
      "print type(bnb_final_vect.vocabulary_)\n",
      "for "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "152034\n",
        "<type 'dict'>\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#takes 16 hours to train. Impractacle\n",
      "pipeline_sgd = Pipeline([('vectorizer', vectorizer), ('transformer', transformer), ('sgd', sgd)]) \n",
      "\n",
      "svmtarget = np.array(targetBinary[0:250000])\n",
      "svmcorpus = np.array(corpus[0:250000])\n",
      "\n",
      "vectorizersvm = CountVectorizer(stop_words ='english', min_df = 1)\n",
      "pipeline_svc = Pipeline([('vectorizer', vectorizersvm), ('transformer', transformer), ('svc', svm)])\n",
      "\n",
      "#pipeline_svc.fit(svmcorpus, svmtarget)\n",
      "\n",
      "score = pipeline_svc.score(npcorpustest, nptargettest)\n",
      "print score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizerTree = CountVectorizer(tokenizer = LemmaTokenizer(), stop_words ='english', min_df = 1, max_features = 50000)\n",
      "text_dtc = tree.DecisionTreeClassifier(max_depth = 10000)\n",
      "#pipeline_text_dtc = Pipeline([('vectorizer', vectorizerTree), ('text_dtc', text_dtc)])\n",
      "\n",
      "#pipeline_text_dtc.fit(corpusMiniTrain, targetBinaryMiniTrain)\n",
      "\n",
      "#score = pipeline_text_dtc.score(corpusSplitTest, targetBinarySplitTest)\n",
      "#print score\n",
      "\n",
      "text_dtc_test = pipeline_text_dtc.predict(text_dtc_test)\n",
      "print classification_report(targetBinaryTest, text_dtc_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-114-0e3c334fedbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipeline_text_dtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vectorizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizerTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'text_dtc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_dtc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpipeline_text_dtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusMiniTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetBinaryMiniTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_text_dtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusSplitTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetBinarySplitTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_mask, X_argsorted, check_input, sample_weight)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Convert data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dense\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Determine output settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_arrays\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0msparse_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dense'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                     raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    264\u001b[0m                                     \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                                     'convert to a dense numpy array.')\n",
        "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizerTree = CountVectorizer(tokenizer = LemmaTokenizer(), stop_words ='english', min_df = 1, max_features = 5000)\n",
      "text_dtc = tree.DecisionTreeClassifier(max_depth = 10000)\n",
      "dictionary = vectorizerTree.fit_transform(corpusMiniTrain)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print vectorizerTree\n",
      "documentTermMatrix_corpusValTest = vectorizerTree.transform(corpusSplitTest)\n",
      "documentTermMatrix_corpusValTest = documentTermMatrix_corpusValTest.toarray()\n",
      "print type(documentTermMatrix_corpusValTest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'numpy.ndarray'>\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SPARSEdocumentTermMatrix_corpusMiniTrain = vectorizerTree.transform(corpusMiniTrain)\n",
      "\n",
      "#print type(SPARSEdocumentTermMatrix_corpusMiniTrain)\n",
      "print type(dictionary)\n",
      "print dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'scipy.sparse.csr.csr_matrix'>\n",
        "  (0, 1463)\t1\n",
        "  (0, 3044)\t1\n",
        "  (0, 2653)\t1\n",
        "  (0, 1965)\t1\n",
        "  (0, 24)\t3\n",
        "  (0, 13)\t3\n",
        "  (0, 2982)\t1\n",
        "  (0, 1528)\t1\n",
        "  (0, 4407)\t1\n",
        "  (0, 159)\t3\n",
        "  (0, 4525)\t1\n",
        "  (0, 3900)\t1\n",
        "  (0, 3201)\t1\n",
        "  (0, 4560)\t1\n",
        "  (0, 2253)\t1\n",
        "  (0, 15)\t1\n",
        "  (0, 16)\t1\n",
        "  (0, 3169)\t1\n",
        "  (0, 1677)\t1\n",
        "  (0, 2314)\t1\n",
        "  (0, 825)\t1\n",
        "  (0, 2127)\t1\n",
        "  (0, 2963)\t2\n",
        "  (0, 4365)\t1\n",
        "  (0, 2144)\t1\n",
        "  :\t:\n",
        "  (375150, 1256)\t1\n",
        "  (375150, 4945)\t1\n",
        "  (375150, 2408)\t1\n",
        "  (375150, 4427)\t1\n",
        "  (375150, 3822)\t1\n",
        "  (375150, 4349)\t1\n",
        "  (375150, 4618)\t1\n",
        "  (375150, 3478)\t1\n",
        "  (375150, 4552)\t1\n",
        "  (375150, 4980)\t1\n",
        "  (375150, 3209)\t1\n",
        "  (375150, 2857)\t1\n",
        "  (375150, 3704)\t4\n",
        "  (375150, 861)\t1\n",
        "  (375150, 3667)\t1\n",
        "  (375150, 2789)\t2\n",
        "  (375150, 1902)\t1\n",
        "  (375150, 3362)\t1\n",
        "  (375150, 1497)\t1\n",
        "  (375150, 3419)\t1\n",
        "  (375150, 2230)\t1\n",
        "  (375150, 3934)\t1\n",
        "  (375150, 518)\t2\n",
        "  (375150, 4080)\t1\n",
        "  (375150, 3526)\t1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_dtc = tree.DecisionTreeClassifier(max_depth = 5000)\n",
      "documentTermMatrix_corpusMiniTrain = dictionary.toarray()\n",
      "print type(documentTermMatrix_corpusMiniTrain)\n",
      "print documentTermMatrix_corpusMiniTrain\n",
      "text_dtc.fit(documentTermMatrix_corpusMiniTrain, targetBinaryMiniTrain)\n",
      "print text_dtc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'numpy.ndarray'>\n",
        "[[ 0  0  0 ...,  0  0  0]\n",
        " [ 0  0  0 ...,  0  0  0]\n",
        " [ 1  0  0 ...,  0  0  0]\n",
        " ..., \n",
        " [10  0  2 ...,  0  0  0]\n",
        " [ 0  0  0 ...,  0  0  0]\n",
        " [ 0  0  1 ...,  0  0  0]]\n",
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=5000, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtc_validate_test = text_dtc.predict(documentTermMatrix_corpusValTest)\n",
      "print classification_report(targetBinarySplitTest, dtc_validate_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.55      0.58      0.57     72727\n",
        "          1       0.55      0.52      0.54     71707\n",
        "\n",
        "avg / total       0.55      0.55      0.55    144434\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "writeTargetFile(dtc_validate_test, \"DecisionTreeText_5000features.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DecisionTreeText_5000features.csv\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#too big to train. Feature set is too large for these conditions\n",
      "text_dtc = tree.DecisionTreeClassifier(max_depth = 5000)\n",
      "pipeline_text_dtc = Pipeline([('vectorizer', vectorizer), ('transformer', transformer), ('text_dtc', text_dtc)])\n",
      "\n",
      "#pipeline_text_dtc.fit(corpusTrain, targetBinaryTrain)\n",
      "\n",
      "score = pipeline_text_dtc.score(corpusSplitTest, targetBinarySplitTest)\n",
      "print score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-109-62cf14b3622c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpipeline_text_dtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vectorizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'text_dtc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_dtc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpipeline_text_dtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetBinaryTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_text_dtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusSplitTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetBinarySplitTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0mdata\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_pre_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 234\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-95-83a93ab0eebc>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLemmaTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     91\u001b[0m     along with :class:`.PunktSentenceTokenizer`).\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     return [token for sent in sent_tokenize(text)\n\u001b[0m\u001b[1;32m     94\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mcurrently\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0monly\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \"\"\"\n\u001b[0;32m--> 743\u001b[0;31m     \u001b[0mresource_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_resource_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# Determine the format of the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mnormalize_resource_url\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nltk:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_resource_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# handled by urllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mnormalize_resource_name\u001b[0;34m(resource_name, allow_relative, relative_path)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mis_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\\\/.]$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'win'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mresource_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    141\u001b[0m     a match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}